<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on TorresNG&#39;s Blog</title>
    <link>/posts/</link>
    <description>Recent content in Posts on TorresNG&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Jul 2019 21:18:06 +0800</lastBuildDate>
    
	<atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>理解MapReduce</title>
      <link>/posts/understanding_mapreduce/</link>
      <pubDate>Mon, 01 Jul 2019 21:18:06 +0800</pubDate>
      
      <guid>/posts/understanding_mapreduce/</guid>
      <description>概念 假如我們有一份很大的數據需要處理，只用單進程單線程需要大量的運行時間，為了減少執行時間，我們可以用多進程多線程并發執行，把一個大的數據划分為獨立較小的數據，然後交給這些進程獨立執行，當這些較小的數據處理完後，我們可以很容易把它們合并成最終結果，這是典型的分治算法，也是MapReduce的主要思想。
當然MapReduce要處理的是大數據的問題，需要解決的事比上面遠大得多。因為單台計算機最後也會出現性能瓶頸，不單擴展單台計算機比較困難且昂貴，而且當單台計算機Crash後，之前所處理的數據全都白費了，所以MapReduce的架構是主要是為了能在多個計算機上實現并行化計算。通過MapReduce可以在大量普通的計算機上實現高性能計算，而且具有可擴展性和高可性的特點。
MapReduce執行部分簡單可以分為單個Master和多個Worker，Master就像公司里的Leader，負責管理，調度和分配任務給小弟，而Worker就是拼了命工作的那些小弟，為了防了單台計算機，這些Worker分布在集群中不同的計算機上執行，當一個計算機Crash後，Master可以將那台計算機所執行的任務分配給其他Worker，這樣實現了高可用。那Master死後怎麼辦？那只能呵呵呵了，當然有各種方式可以解決，其中一種就是通知用戶Master Crash了，讓用戶決定處理方法。
金map 和 reduce 通過MapReduce，我們可以不用處理分布式上遇到的各種問題，MapReduce為我們處理了并行編程中分布存儲、工作調度、負載均衡、容錯處理及網絡通信等複雜問題，MapReduce任務分為兩個阶段：map阶段和reduce阶段。每阶段都以鍵值對作為輸入和輸出，其類型由程序員選擇。程序員還需要寫兩個函數：map函數和reduce函數。map函數負責把每個分片進行處理，reduce函數負責把map處理後的結果汇總起來。需要注意的是，用MapReduce來處理的數據集（或任務）必須具備這樣的特點：待處理的數據集可以分解成計多小的數據集，而且每一個小數據集都可以完全并行地處理。
分片 Hadoop將MapReduce的輸入數據划分成等長的小數據塊，稱為輸入分片或分片。Hadoop為每個分片构建一個map任務，并由該任務來運行用戶自定義的map函數從而處理分片中的每條記錄。
最佳分片的大小應該與塊大小相同：因為它是確保可以存儲在單個節點上的最大輸入塊的大小。如果分片跨越兩個數據塊，那麼對於任何一個 HDFS 節點，基本上都不可能同時存儲這兩個數據塊。
數據流 map任務將其輸出寫入本地硬盤，而非HDFS。因為map的輸出是中間結果，如果把它存儲在HDFS中并實現備份，難免有些小題大做。如果運行map任務的節點在將map中間結果傳送給reduce任務之前失敗，Hadoop將在另一個節點上重新運行這個map任務以再次构建map中結果。
而reduce任務不具備數據本地化的優勢，單個reduce任務的輸入通常來自於所有mapper的輸出。reduce的輸出通常存儲在HDFS中以實現可靠存儲，對於reduce輸出的每個HDFS塊，第一個複本存儲在本地節點上，其他複本出於可靠性考慮存儲在其他機架的節點中。因此，將reduce的輸出寫入HDFS確實需要占用網絡帶寬，但這與正常的HDFS管線寫入的消耗一樣。
如果有好多個reduce任務，每個map任務就會針對輸出進行分區，即為每個reduce任務建一個分區。每個分區有許多鍵（及其對應的值），但每個鍵對應的鍵-值對記錄都在同一分區中。分區可由用戶定義的分區函數控制，但通常用默認的partitioner通過哈希函數來分區，很高效。map任務和reduce任務之間的數據流稱為shuffle，因為每個reduce任務的輸入都來自許多map任務。
當數據處理可以完全并行（即無需shuffle時），可能會出現無reduce任務的情況。在這種情況下，唯一的非本地節點數據傳輸是map任務將結果寫入HDFS。
combiner函數 集群上的可用帶寬限制了MapReduce作業數量，因此䀆量避免map和reduce任務之間的數據傳輸是有利的（例如傳輸較小的數據）。Hadoop允許用戶針對map任務的輸出指定一個combiner（就像mapper和reduce一樣），combiner函數的輸出作為reduce函數的輸入。由於cmobiner屬於優化方案，所以Hadoop無法確定要對一個指定的map任務輸出記錄調用多少次combiner（如果需要）。換而言之，不管調用combiner多少次，0次，1次或多次，reducer的輸出結果都是一樣的。
運行代碼 安裝 Java 和 Hadoop 可以參考我另一篇文章 Install Hadoop 2.7 in CentOS 7。
這里我用 Hadoop權威指南 第二章的代碼做測試，源代碼可以從這里下載。這個代碼計算每年的最高氣溫。
用 git 下載源碼下來後，將數據 sample.txt 複製到源碼路徑下，然後編譯運行。最後輸出結果。
$ git clone https://github.com/tomwhite/hadoop-book.git $ cd hadoop-book $ cp input/ncdc/sample.txt ch02-mr-intro/src/main/java/ $ cd ch02-mr-intro/src/main/java/ $ javac MaxTemperature.java Note: MaxTemperature.java uses or overrides a deprecated API. Note: Recompile with -Xlint:deprecation for details.</description>
    </item>
    
    <item>
      <title>Install Hadoop 2.7 in CentOS 7</title>
      <link>/posts/install_hadoop_2.7_in_centos_7/</link>
      <pubDate>Sun, 30 Jun 2019 16:10:21 +0800</pubDate>
      
      <guid>/posts/install_hadoop_2.7_in_centos_7/</guid>
      <description>配置環境 剛裝完 CentOS 7 先連網，然後安裝 wget, net-tools( ifconfig, netstat ) 和 vim 工具
$ sudo dhclient $ sudo yum install -y wget net-tools vim  在 /opt/ 目錄下創建 module 目錄和 software 目錄，然後分配這兩個目錄的權限給自己
$ cd /opt/ $ sudo mkdir module software $ sudo chown torres:torres module/ software/ $ ll total 0 drwxr-xr-x. 2 torres torres 6 Jun 30 04:59 module drwxr-xr-x. 2 torres torres 6 Jun 30 04:59 software  安裝Java JDK 在 software 目錄中下載 Java OpenJDK，解壓到 module 目錄下，然後在 /etc/profile 將 Java 添加到環境變量。需要 OpenJDK 其他版本可以到 這里下載</description>
    </item>
    
    <item>
      <title>My First Post</title>
      <link>/posts/my-first-post/</link>
      <pubDate>Sun, 30 Jun 2019 15:58:56 +0800</pubDate>
      
      <guid>/posts/my-first-post/</guid>
      <description>由於之前的Blog源碼不見了，所以可以有籍口創建新的Blog了。😂
不管再怎麼累，也要保持學習、折騰和技術分享，把自己學到東西分享出來。</description>
    </item>
    
  </channel>
</rss>